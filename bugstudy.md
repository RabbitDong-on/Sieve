|Num|Source|Date|Severity|Status|Implict|Input|Hardware Root Cause(HRC)|Software Root Cause(SRC)|Interaction among input, HRC and SRC|Link| 
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|1|zookeeper-710|18/Mar/10-20/Mar/10|Blocker|Fixed|client operation failed|client connection&normal operation|fail-slow NIC|server cannot handle sessionmoved exception|client connects to follower1 that encounters fail-slow hardware. client timeouts and successfully connects to leader. At this time, follower1 finish processing the connection request and sends renew to leader. Leader think the session moved and does not process client requests.|https://issues.apache.org/jira/browse/ZOOKEEPER-710|
|2|zookeeper-2251|19/Aug/15-27/Jul/18|Critical|Fixed|client hang|client send sync operation|fail-slow NIC|client sync operation wait infinitely without ack|client executes sync network operations.The ack is blocked by the fail-slow NIC.|https://issues.apache.org/jira/browse/ZOOKEEPER-2251|
|3|zookeeper-3296|08/Mar/19-14/Jun/19|Major|Fixed|node cannot join quorum|client connection&restart node|fail-slow disk|connection requests in a FIFO queue. The first connection request timeouts, leading to  later requests timeouts. This effect will be accumulated.|many clients connect to server1. restart other nodes. Server1 timeouts from these client connections. Hence, connections from other servers also timeout.|https://issues.apache.org/jira/browse/ZOOKEEPER-3296|
|4|zookeeper-4074|17/Jan/21-01/Jul/21|Critical|Confirmed|node hang|client normal operation|fail-slow NIC|one thread is response for writePacket, readPacket.|follower sends packet to leader. It is stuck due to fail-slow NIC. This thread cannot switch to readPacket to check the status.|https://issues.apache.org/jira/browse/ZOOKEEPER-4074|
|5|zookeeper-2886|31/Aug/17-10/Jul/18|Major|Fixed|client operation failed|client multiop|fail-slow NIC|server cannot handle sessionmoved exception for client multiops|client connects to follower1 that encounters fail-slow hardware. client timeouts and successfully connects to leader. At this time, follower1 finish processing the connection request and sends renew to leader. Leader think the session moved and does not process client multiops.|https://issues.apache.org/jira/browse/ZOOKEEPER-2886|
|6|zookeeper-2219|24/Jun/15-22/Nov/18|Major|Confirmed|fixed by zk-2886|-|-|-|-|https://issues.apache.org/jira/browse/ZOOKEEPER-2219|
|7|zookeeper-2201|02/Jun/15-07/Jun/15|Critical|Fixed|client operation failed|client normal operation|fail-slow NIC|blocking IO operation is within the synchronized statement|leader sends snapshot to follower slowly due to fail-slow NIC. Client update-type operations require lock hold by sync IO. Hence, these client operation failed.|https://issues.apache.org/jira/browse/ZOOKEEPER-2201|
|8|zookeeper-3531|03/Sep/19-09/Oct/19|Critical|Fixed|client operation failed|client normal operation|fail-slow NIC|serializing acl cache within the synchronized statement|leader sends acl to follower slowly due to fail-slow NIC. Client update-type operations require lock hold by sync IO. Hence, these client operation failed.|https://issues.apache.org/jira/browse/ZOOKEEPER-3531|
|9|zookeeper-4293|12/May/21-13/Oct/23|Critical|Confirmed|client hang|client normal operation|fail-slow NIC|two event in queue. The first event need a lock hold by the second event.|client connects to server1 that encounters fail-slow NIC. Client timeouts and connects to normal server2.The cleanup operation from the first connection request hold the lock required by the second connection request.|https://issues.apache.org/jira/browse/ZOOKEEPER-4293|
|10|zookeeper-417|27/May/09-27/Jun/09|Blocker|Fixed|client gets inconsistent result|client normal operation|fail-slow NIC|timeout request can still be processed by the cluster|client sends setData /a 1 to follower that encounters fail-slow NIC. Client timeouts and sends setData /a 2 to leader. The second request is finished before the first one. The result is overwritten.|https://issues.apache.org/jira/browse/ZOOKEEPER-417|
|11|zookeeper-642|13/Jan/10-11/May/12|Major|Fixed|generate lots of useless logs|client normal operation|fail-slow NIC|timeout threhold of select is too small(10ms)|client sends requests to server1 that encounters fail-slow NIC. The selector continues timeouting and generates lots of useless logs|https://issues.apache.org/jira/browse/ZOOKEEPER-642|
|12|hdfs-5032|25/Jul/13-27/Oct/15|Major|Fixed|normal node is excluded from the cluster|client normal operation|fail-slow disk|same as hdfs-9178|-|https://issues.apache.org/jira/browse/HDFS-5032|
|13|hdfs-9178|01/Oct/15-07/Oct/15|Critical|Fixed|normal node is excluded from the cluster|client normal operation|fail-slow disk|upstream node cannot find the real reason of disconnection from downstream node|upstream node is stuck due to fail-slow disk, the downstream node timeouts and disconnects. The error handler of upstream node judges that downstream node encounter error.|https://issues.apache.org/jira/browse/HDFS-9178|
|14|hdfs-4859|28/May/13-30/May/13|Major|Confirmed|Namenode hang|client normal operation|fail-slow disk|It is absent of explicit timeout in FileJournalManager.|FileJournalManager is stuck due to fail-slow disk. It makes namenode unresponsive for long time.|https://issues.apache.org/jira/browse/HDFS-4859|
|15|hdfs-5522|16/Nov/13-13/May/14|Major|Fixed|node hang|client normal operation|fail-slow disk|error handler cannot recognize the timeout comes from disk or network|fail-slow disk cause a read opertion timeout. However, checkDiskError() will not be invoked.|https://issues.apache.org/jira/browse/HDFS-5522|
|16|hdfs-7065|16/Sep/14-19/Sep/14|Critical|Fixed|the replica is corrupted|client normal operation|fail-slow disk|recoverClose() is not thread-safety|client tries to close a block, but timeouts.The pipeline encounters fail-slow disk. Finally, there is a data race between the first and second recovery process. Due to non-thread-safety recoverClose(), the only replica is corrupted.|https://issues.apache.org/jira/browse/HDFS-7065|
|17|hdfs-7489|09/Dec/14-10/Dec/14|Critical|Fixed|datanode hang|client normal operation|fail-slow disk|checkDiskError() can hold a lock, then block other threads.|datanode executes checkDiskErrors() and encounters fail-slow disk. checkDiskErrors() hold a lock for entire FsVolumeList. This lock makes datanode hang.|https://issues.apache.org/jira/browse/HDFS-7489|
|18|hdfs-10301|18/Apr/16-18/Oct/16|Critical|Fixed|normal block is removed|client normal operation|fail-slow disk|there is a data race between two block report processes|Datanode timeout sending a block report to namenode that encounter fail-slow disk. Datanode sends the block report again. Namenode interleave processing storages from different reports. The block reportid is overwritten. There is a inconsistency among different block. Some blocks are considered as zombie and removed.|https://issues.apache.org/jira/browse/HDFS-10301|
|19|hdfs-12645|12/Oct/17~|Major|Confirmed|severe performance degradation and may cause missing blocks.|client normal operation|fail-slow disk|bad locking practices|A slow disk will cause pipelines to experience significant latency and timeouts, increasing lock/io contention.Sometimes, the node may be incorrectly declared as dead.|https://issues.apache.org/jira/browse/HDFS-12645|
|20|hdfs-13111|6/Feb/18~|Critical|Confirmed|block is corrupted|client normal operation|fail-slow disk|there is a data race between increased block report and recoveryclose processes|The last datanode finishes the last packet and sends ack to client. The client closes the pipeline. The close request timeouts since the last data node encounters fail-slow disk. If the recovery close finsihes before IBR, the generation stamp is overwritten.|https://issues.apache.org/jira/browse/HDFS-13111|
|21|hdfs-13359|28/Mar/18-10/Aug/19|Major|Fixed|DataXceiver hang|client normal operation|fail-slow disk|blocking IO operation is within the synchronized statement|getBlockInputStream hold the lock and the disk becomes fail-slow. getBlockInputStream blocks DataXceiver.|https://issues.apache.org/jira/browse/HDFS-13359|
|22|hdfs-9659|18/Jan/16-01/Feb/16|Critical|Fixed|Standby namenode|client normal operation|fail-slow disk|there is no timeout to protecting EditLogTailorThread::rollEditLog()|SNN invoke rollEditLog rpc to ANN. ANN is stuck due to fail-slow disk. The nameservice cannot switch to SNN.|https://issues.apache.org/jira/browse/HDFS-9659|
|23|hdfs-11755|05/May/17-11/May/17|Major|Fixed|block with underconstruction status is considered as missed block|client normal operation|fail-slow disk->fail-stop disk|internal checker uses the wrong standard to judge|the last datanode in pipeline encounter fail-slow disk. The first and second datanode timeouts. The write requests is finished in the last datanode. This datanode sends IBR to NN. When the disk in the last datanode fails, NN declare the block missing because there are no other replicas.|https://issues.apache.org/jira/browse/HDFS-11755|
|24|hdfs-5341|11/Oct/13-24/Oct/13|Major|Fixed|datanode hang|client normal operation|fail-slow disk|DirectoryScanner holds a lock|DirectoryScanner holds a lock, but the disk becomes fail-slow. Heartbeat thread and all DataXceiver threads will be blocked.|https://issues.apache.org/jira/browse/HDFS-5341|
|25|hdfs-5016|21/Jul/13-26/Jul/13|Blocker|Fixed|datanode is marked dead|client normal operation|fail-slow NIC|there is no timeout in responder thread|client request timeouts in pipeline. When pipeline recover, FSDataset recoverRbw is holding the lock and is stuck waiting on join for the responder thread. However, the responder is stuck in fail-slow nic.|https://issues.apache.org/jira/browse/HDFS-5016|
|26|hdfs-3493|02/Jul/12-15/Sep/14|Major|Fixed|block cannot be replicated even when there are available nodes|client normal operation|fail-slow NIC|datanode set block status wrongly.|There are four DNs. DN3,4 encounter fail-slow nic. client send write request. Pipeline DN1,2,3 timeouts. Then,pipeline DN1,2,4 timeouts. Only DN1,DN2 finish commitblocksync. NN requires to replicate the block due to the number of replicas is less than 3. However, DN3,4 reject replicaion request. Because, the status of the block in DN3,4 is set to RBW.|https://issues.apache.org/jira/browse/HDFS-3493|
|27|hdfs-15235|24/Mar/20~|Major|Patch available|node hang|NN failover|fail-slow NIC|haadmin command fence a standby namenode|When active NN is unhealthy, the admin manually failover to NN2 by running haadmin command. After NN2 receive command and becomes active, admin node encounters fail-slow nic and fails to receive ack to the admin. The amdin command fences NN2. Finally, there is no healthy namenode in cluster.|https://issues.apache.org/jira/browse/HDFS-15235|
|28|hdfds-4699|16/Apr/13-19/Apr/13|Major|Fixed|unit test failed|NN failover|client normal operation|fail-slow NIC|miscategorizing some network failures as disk failures|when NN failovers, the procedure encounters fail-slow nic causing exception. The exception is miscategorizing as disk failures. Then the diskchecker is invoked.|https://issues.apache.org/jira/browse/HDFS-4699|
|29|hdfs-16659|13/Jul/22-07/Sep/22|Critical|Fixed|NN cannot reply editlogs|client normal operation|fail-slow NIC|JournalNode does not throw NewerTxnIdException if SinceTxId is bigger than HighestWrittenTxId|NN sync editlog(0~21) to JN0 and timeouts due to fail-slow nic in JN0. NN sync editlog(0~30) to JN1 and JN2. NN getJournaledEdits from JN0,1,2. However, NN only get editlogs from JN1.JN0 does not contain and throw any exceptions. JN2 does not reply due to fail-slow nic.|https://issues.apache.org/jira/browse/HDFS-16659|
|30|hbase-12270|16/Oct/14-16/Jan/15|Critical|Fixed|Get an unexpected exception|client normal operation|fail-slow disk|when cache is full, the write request is dropped.|The bucket cache encounters fail-slow disk. Hence cache write is slow. The write queue is full quickly. The incoming requests will be abandoned.|https://issues.apache.org/jira/browse/HDFS-12270|
|31|hbase-10833|24/Mar/14-26/Mar/14|Major|Fixed|node hang|client normal operation|fail-slow NIC|The number of retry is exhausted quickly|One a RS is in failed server list due to fail-slow nic, AM quickly exhausted all retries.(sendRegionOpen)|https://issues.apache.org/jira/browse/HDFS-10833|
|32|hbase-10895|02/Apr/14-05/Apr/14|Major|Fixed|node hang|client normal operation|fail-slow NIC|The number of retry is exhausted quickly|One a RS is in failed server list due to fail-slow nic, AM quickly exhausted all retries.(sendRegionClose)|https://issues.apache.org/jira/browse/HDFS-10895|
|33|hbase-26195|13/Aug/21-09/Sep/21|Major|Fixed|data is inconsistent among nodes|client normal operation|fail-slow disk|rollback mechanism does not roll back all replicas|wal synchronization timeouts. Client receives an exception and rolls back data in primary cluster. But data is present in replica cluster.|https://issues.apache.org/jira/browse/HDFS-26195|
|34|hbase-27947|23/Jun/23-18/Aug/23|Critical|Fixed|regionserver out of memory|client normal operation (ssl enabled)|fail-slow disk|each rpc handler ocupies a buffer|multi client connection rpc is blocked since server encounters fail-slow disk. When the size of ocupied buffer exceeds MaxDirectMemorySize, the OOM will arise.|https://issues.apache.org/jira/browse/HDFS-27947|
|35|hbase-11536|17/Jul/14-26/Aug/14|Critical|Fixed|inconsistent data exists in metatable|client normal operation|fail-slow disk|Timestamp is absent|Hmaster opens region in region server1 and stores metatable in RSm. Storing metatable timeouts. HM stores metatable(E2) again in RSm. Then openning region in RS1 timeouts. HM opens region in RS2 and stores new metatable in RSm successfully. Then E2 finishes and overwritten lastest metatable.|https://issues.apache.org/jira/browse/HDFS-11536|
|36|hbase-13430|09/Apr/15-18/Apr/15|Critical|Fixed|useful HFile is deleted|clone a snapshot into a new table|fail-slow disk|do not consider the status of talbe under tempdir.|cloning a snapshot is stuck since node encounters a fail-slow disk. The table under tempdir refer a hfile of snapshot. HFileLink does not consider table under tempdir and delete the hfile.|https://issues.apache.org/jira/browse/HDFS-13430|
|37|hbase-14498|28/Sep/15-16/Dec/23|Blocker|Patch available|Master is stuck in infinite loop|client normal operation|fail-slow NIC|active Hmaster cannot connect to ZK cluster. But still process the request from region servers.|Active Hmaster cannot connect to any zookeeper server due to encoutered fail-slow nic. Hence, there is no session timeout. ZK cluster will not abort HM1. HM2 connects to ZK cluster, but no RS send request to HM2.|https://issues.apache.org/jira/browse/HDFS-14498|
|38|hbase-26256|06/Sep/21~|Major|Confirmed|client hang|client normal operation|fail-slow NIC|client should not see the table under creating|a Region server is stuck in creating table due to fail-slow nic. However, this pending table can be seen by the client. The client`s operation based on this table is blocked.|https://issues.apache.org/jira/browse/HDFS-26256|
|39|hbase-27520|07/Dec/22~|Major|same as hbase-26256|-|-|-|-|-|https://issues.apache.org/jira/browse/HDFS-27520|
|40|mapreduce-2177|07/Nov/10~|Major|Confirmed|slow task is killed|client normal operation|fail-slow disk|slow task should be handled by speculation|Task write large data into  disk. There is a timeout to monitor this procedure. It is normal to take long time to finish writing. If the time exceeds timeout value, the task suicides. If task write small data into fail-slow disk. This task also suicides. The timeout mechanism can not recoginze the true reason.|https://issues.apache.org/jira/browse/MAPREDUCE-2177|
|41|mapreduce-3121|29/Sep/11-30/Nov/11|Blocker|Fixed|job failure|client normal oepration|fail-slow disk|NodeManager cannot automatically offline fail-slow disk|When the TaskTracker encounter fail-slow disk, the job will fail. The TaskTracker and NodeManager cannot recognize fail-slow disk.|https://issues.apache.org/jira/browse/MAPREDUCE-3121|
|42|mapreduce-7369|19/Nov/21-20/Jun/22|Major|Fixed|high latency in closing a stream|client normal operation|fail-slow disk|timeout cannot handle multifile close.|If close operation takes long time, the timeout mechanism cannot recoginze the true reason. It may treat normal spent time as exception caused by fail-slow disk.|https://issues.apache.org/jira/browse/MAPREDUCE-7369|
|43|mapreduce-1800|20/May/10-22/May/10|Major|Confirmed|map node can be blacklisted|client normal operation|fail-slow NIC|use map output fetch failures to blacklist nodes|if few reducer encounter fail-slow nic, they will collectively cause nodesto be blacklisted.|https://issues.apache.org/jira/browse/MAPREDUCE-1800|
|44|cassandra-1434|26/Aug/10-28/Sep/10|Normal|Fixed|cluster performance degradation|client normal operation|fail-slow disk|one coordinator blocks many write request|one coordinater encounters fail-slow disk and receives a large number of write requests. Then, the coordinator unblocks all requests at the same time causing GC or compaction on every node.|https://issues.apache.org/jira/browse/CASSANDRA-1434|
|45|cassandra-16143|24/Sep/20-09/Dec/20|Normal|Fixed|Streaming fails|client normal operation|fail-slow disk|internode_tcp_user_timeout is not suitable for streaming|we bootstrap 3 node cluster. If one node encounter fail-slow disk, make the streaming slow. It takes long time that exceeds internode_tcp_user_timeout. Streaming fails. However, internode_tcp_user_timeout is used for various goals,e.g., dead node detection.|https://issues.apache.org/jira/browse/CASSANDRA-16143|
|46|cassandra-14358|31/Mar/18-02/Nov/18|Normal|Fixed|node hang|restart&ssl connection|fail-slow NIC|there is no timeout protecting socketWrite0|node1 restarts. node2 retries to connect to the node1 but encounter fail-slow nic. There is no timeout protecting this connection. Hence node2 is stuck in socketWrite0.|https://issues.apache.org/jira/browse/CASSANDRA-14358|
|47|cassandra-15700|07/Apr/20-30/Jun/20|Normal|Fixed|performance degradation|client normal operation|fail-slow NIC|prune frequently|Messages among nodes will be store in the queue.If a node encounter fail-slow nic, it makes the first message timeout causing remaining message timeout. Then prune procedure will be invoked frequently |https://issues.apache.org/jira/browse/CASSANDRA-15700|
|48|cassandra-6415|28/Nov/13-03/Dec/13|Normal|Fixed|node hang|client normal operation|fail-slow NIC|snapshot response message is droppable by MessagingService|If a node encounter a fail-slow nic, the coming snapshot response messages is dropped or delayed. The following repair operations are blocked forever.|https://issues.apache.org/jira/browse/CASSANDRA-6415|
